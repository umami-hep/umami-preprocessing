{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"UPP: Umami Preprocessing","text":"<p>Welcome to the Umami PreProcessing (UPP) package, a modular preprocessing pipeline for jet tagging. UPP is used to prepare datasets for training various taggers.  In particular, it handles hybrid sample creation, resampling, normalisation, and shuffling.</p> <p>The code is hosted on the Github:</p> <ul> <li>https://github.com/umami-hep/umami-preprocessing</li> </ul> <p>You can find information about tagger training and FTAG software at the central docs pages.</p> UPP tutorial <p>A tutorial on how to use the framework is provided at the central FTAG docs page</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Input ntuples for the preprocessing are produced using the training-dataset-dumper which converts from ROOT files to HDF5 ntuples. A list of available h5 ntuples is maintained in the central FTAG documentation pages. However, the ntuples listed there are not directly suitable for algorithm training and require preprocessing (handled by this package).</p> <p>This library is alredy used to preprocess data for Salt framework. UPP is planned to be integrated into Umami framework for training of Umami/DIPS and DL1r and replace current umami preprocessing, as it addresses several issues with the current umami preprocessing workflow, and uses the <code>atlas-ftag-tools</code> package extensively.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>The primary motivation behind preprocessing the training samples is to ensure that the distributions of kinematic variables such as p_T and \\eta\\eta are the same for all flavors. This uniformity in kinematic distributions is crucial to avoid kinematic biases in the tagging performance. In order to ensure the uniformity in kinematic distributions, resampling techniques are employed. These techniques involve removing samples from the majority class (under-sampling) and/or adding more samples from the minority class (over-sampling).</p> <p>The preprocessing can also be used to control the number of jets of each flavour, to stitch together jets from various samples, and to perform the shuffling and normalisation.</p>"},{"location":"#hybrid-samples","title":"Hybrid Samples","text":"<p>Umami/DIPS and DL1r are trained on so-called hybrid samples created by combining t\\bar{t}t\\bar{t} and Z'Z' jets using a p_Tp_T threshold, which is defined by the <code>pt_btagJes</code>. Below a certain pt threshold (which needs to be defined for the preprocessing), t\\bar{t}t\\bar{t} events are used in the hybrid sample. Above this pt threshold, the jets are taken from Z'Z' events. The advantage of these hybrid samples is the availability of sufficient jets with high pt, as the t\\bar{t}t\\bar{t} samples typically have lower-pt jets than those jets from the Z'Z' sample.</p> <p>The following image show the distributions of jet flavours in both samples</p> <p></p> <p>After applying <code>pdf</code> resampling with upscaling, we achieve the following combined distributions for jets:</p> <p></p> <p>It's worth noting that, while we used t\\bar{t}t\\bar{t} and Z'Z' samples here for illustrative purposes, you can use any type of samples. Additionally, you're not obligated to create a hybrid sample; UPP can still be used with a single sample for preprocessing.</p>"},{"location":"#this-package","title":"This package","text":"<p>Compared with umami, the main features of this code are:</p> <ul> <li>modular, class-based design</li> <li>use of h5 virtual datasets to wrap the source files</li> <li>only 2 main stages: resample -&gt; merge -&gt; done!</li> <li>parallelised processing of flavours within a sample, which avoids wasted reads</li> <li>support for different resampling \"regions\", which is useful for generalising to Xbb preprocessing</li> <li>n-dim sampling support, which is also useful for Xbb</li> <li>\"new\" improved training file format (which is actually just the tdd output format)<ul> <li>structured arrays are smaller on disk and therefore faster to read</li> <li>only one dataloader is needed and can be reused for training and testing</li> <li>other plotting scripts can support a single file format</li> <li>normalisation/concatenation is applied on the fly during training</li> <li>training files can contain supersets of variables used for training</li> </ul> </li> <li>new \"countup\" samping which is more efficient than pdf (it uses more the available statistics and reduces duplication of jets)</li> <li>the code estimates the number of unique jets for you and saves this number as an attribute in the output file</li> </ul> <p>These features yield the following benefits as compared with umami:</p> <ul> <li>only one command is needed to generate all preprocessing outputs (running with <code>--split=all</code> will produce train/val/test files)</li> <li>lines of code are reduced vs umami by 4x</li> <li>10x faster than default umami preprocessing (0.06 vs 0.825 hours/million jets in an old test)</li> <li>improvements to output file size and read speed</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>The configuration of the preprocessing is done with a <code>.yaml</code> file which steers the whole preprocessing. Available example config files for UPP can be found in <code>upp/configs</code>.</p> <p>Each aspect of the configuration is described in detail below.</p>"},{"location":"configuration/#input-h5-samples","title":"Input H5 Samples","text":"<p>Here we define the input h5 samples which are to be preprocessed. Each sample is defined using one or more DSIDs, which generally come from the training-dataset-dumper. If a list of DSIDs is provided, jets from each DSID will be merged according to the <code>equal_jets</code> flag (see below). The samples are used to define components later on in configs and so one should define them with anchors.</p> <p>Below is an example and a table explaining each setting.</p> Single DSIDMultiple DSIDs <pre><code>ttbar: &amp;ttbar\nname: ttbar\npattern: name1.*.410470.*/*.h5\n</code></pre> <pre><code>ttbar: &amp;ttbar\nname: ttbar\nequal_jets: False\npattern:\n- name1.*.410470.*/*.h5\n- name2.*.410470.*/*.h5\n</code></pre> Setting Type Explanation Default <code>name</code> <code>str</code> The name of the sample, used in output filenames. Required <code>pattern</code> <code>str</code> or <code>list[str]</code> A single pattern or a list of pattern that match h5 files in a downloaded dataset. H5 files matching each pattern will be transparently merged using virtual datasets. Required <code>equal_jets</code> <code>bool</code> Only relevant when providing a list of patterns. If <code>True</code>, the same number of jets from each DSID are selected. This is required for e.g. in Xbb QCD where each DSID belongs to a different slice, and the resampling would break if you tried to resample with one or more slices missing. If <code>False</code> this is not enforced, allowing for larger numbers of available jets. <code>True</code>"},{"location":"configuration/#global-cuts","title":"Global Cuts","text":"<p>The selections that should be applied to all the data should be listed under <code>common:</code>. For example these could be outlier removal cuts, or a global kinematic selection. To do this one first provides the variable name (<code>str</code>), then the comparison operator (<code>str</code>) and a number to compare to (<code>int</code>, <code>float</code> or <code>list</code>). Possible operators are:</p> <ul> <li><code>\"==\"</code>, <code>\"!=\"</code>, <code>\"&lt;=\"</code>, <code>\"&gt;=\"</code>, <code>\"&gt;\"</code>, <code>\"&lt;\"</code> which work the same as in python.</li> <li><code>\"in\"</code> and <code>\"notin\"</code> to check if the value is in the list.</li> <li><code>\"%{i}==\"</code>, <code>\"%{i}==\"</code>, <code>\"%{i}==\"</code> operators to compare the modulo w.r.t. <code>i</code> of an integer. </li> </ul> <p>Along with the common selection cuts, you should also specify the cuts that separate <code>train</code>, <code>val</code> and <code>test</code> splits using modulo of <code>eventNumber</code>. For example:</p> <pre><code>global_cuts:\ncommon:\n- [JetFitterSecondaryVertex_mass, \"&lt;\", 25000]\n- [JetFitter_deltaR, \"&lt;\", 0.6]\ntrain:\n- [eventNumber, \"%10&lt;=\", 7]\nval:\n- [eventNumber, \"%10==\", 8]\ntest:\n- [eventNumber, \"%10==\", 9]\n</code></pre> More info about cuts <p>The <code>Cuts</code> class is defined in the <code>atlas-ftag-tools</code> package.</p> k-fold training selection <p>If you are training a model that will be used in production, you may need to worry about overtraining. A variable <code>jetFoldHash</code> is included in newer h5 dumps which allows you to independent models on different folds of the data. If you are just performing studies, then don't worry about applying any selections on the <code>jetFoldHash</code>,  since the train/val/test split will suffice.</p>"},{"location":"configuration/#resampling-regions","title":"Resampling Regions","text":"<p>Next we define any kinematic regions which need to be resampled separately, again using anchors as these will also be used in the definition of our components. For each region you need to provide a name and a list of cuts (see above). Here is an example:</p> <pre><code>lowpt: &amp;lowpt\nname: lowpt\ncuts:\n- [pt_btagJes, \"&gt;\", 20_000]\n- [pt_btagJes, \"&lt;\", 250_000]\nhighpt: &amp;highpt\nname: highpt\ncuts:\n- [pt_btagJes, \"&gt;\", 250_000]\n- [pt_btagJes, \"&lt;\", 6_000_000]\n</code></pre> <p>Again, aliasing these just helps to reduce duplication of information when defining the components as can be seen below.</p>"},{"location":"configuration/#components","title":"Components","text":"<p>The <code>components</code> section is where all the configuration comes together.  A component is a combination of a region, a sample and a flavour. They allow for full flexibility when defining different preprocessing pipelines (e.g. single-b versus Xbb).</p> <p>An example <code>components</code> block is provided below.</p> <pre><code>components:\n- region:\n&lt;&lt;: *lowpt\nsample:\n&lt;&lt;: *ttbar\nflavours: [bjets, cjets, ujets]\nnum_jets: 10_000_000\n\n- region:\n&lt;&lt;: *highpt\nsample:\n&lt;&lt;: *zprime\nflavours: [bjets, cjets, ujets]\nnum_jets: 5_000_000\n</code></pre> <p>Notice that we use <code>&lt;&lt;*</code> insertion tool to insert already defined regions and samples.</p> Setting Type Explanation <code>region</code> anchor The pre-defined kinematic region anchor, e.g. <code>lowpt</code> or <code>highpt</code>, or <code>inclusive</code> if not splitting in p_T <code>sample</code> anchor The pre-defined sample anchor, e.g. t\\bar{t}t\\bar{t} or Z'Z' <code>flavours</code> <code>list[str]</code> One or more jet flavours, e.g. <code>[bjets]</code> or <code>[ujets]</code>. The list syntax is pure syntactic sugar. If more then one is provided, separate components are created for each flavour. <code>num_jets</code> <code>int</code> The number of jets to be sampled from this component in the training split <code>num_jets_val</code> <code>int</code> Optional (default: <code>num_jets//10</code>) number of jets of this component in validation set. <code>num_jets_test</code> <code>int</code> Optional (default: <code>num_jets//10</code>) number of jets of this component in a test set."},{"location":"configuration/#variables","title":"Variables","text":"<p>The next thing you need is to provide the variables that are taken from the TDD files and written in the resampled dataset. Selecting only a subset of variables keeps the output files lightweight, and ensures the dataloading does not become a bottleneck during training.</p> <p>One can simply define them under <code>variables:</code> like:</p> <p><pre><code>variables:\njets:\ninputs:\n- pt_btagJes\n- absEta_btagJes\nlabels:\n- HadronConeExclTruthLabelID\n- pt\n- eta\n\ntracks:\ninputs:\n- dphi\n- deta\n- qOverP\n- IP3D_signed_d0_significance\n- IP3D_signed_z0_significance\nlabels:\n- ftagTruthOriginLabel\n- ftagTruthVertexIndex\n</code></pre> Each key under <code>variables:</code> corresponds to a dataset name in the TDD h5 file (e.g. <code>jets</code>, <code>tracks</code>, <code>hits</code>). The combined set of variables in <code>inputs</code> and <code>labels</code> are carried over to the output files to a dataset with the same name as the input dataset. Internally, UPP will compute normalisation parameters for variables in the <code>inputs</code>, and compute class weightings (for categorical labels) for variables in the <code>labels</code> block.</p> <p>Alternatively include the variables from your custom variable config by providing the full path to the file after an include statement. The file you provide should have the same structure as shown above but without <code>variable:</code> level. For example: <pre><code>variables: !include xbb-variables.yaml\n</code></pre></p> <p>One can also import vaiables configs already provided in this package <code>upp/config/</code> yaml files using just the yaml file name e.g.:</p> <pre><code>variables: !include /&lt;full path to your file&gt;.yaml\n</code></pre> You can choose later which variables in your output files are used for training <p>When it comes to defining your training config, you will be required to define the variables used for training. So it's okay to include here input variables you are not sure whether you will need, for example when testing the importance of different inputs. This is straightforward since we always store data using structured arrays (in the same format as the TDD outputs).</p>"},{"location":"configuration/#track-selections","title":"Track selections","text":"<p>You can apply on the fly selections to tracks in the preprocessing stage (specifically the merging step).</p> <p>To do this, include a <code>selection</code> key in the variable config block under the tracks, for example:</p> <pre><code>  tracks:\n    inputs:\n      - d0\n    labels:\n      - ftagTruthOriginLabel\n    selection:\n      - [d0, \"&gt;\", 0.1]\n</code></pre>"},{"location":"configuration/#resampling","title":"Resampling","text":"<p>There are currently two resampling methods implemented in the package <code>pdf</code> and <code>countup</code> and they share most of setting. Below is the example of setting up the <code>pdf</code> resampling method and a table describitng all the parameters.</p> <p>In order to run UPP without any kinematic resampling, just set <code>method: none</code>.  Note you will still need to run the resampling stage of the preprocessing pipeline.</p> <pre><code>resampling:\ntarget: cjets\nmethod: pdf\nupscale_pdf: 2\nsampling_fraction: auto\nvariables:\npt_btagJes:\nbins: [[20_000, 250_000, 50], [250_000, 1_000_000, 50], [1_000_000, 6_000_000, 50]]\nabsEta_btagJes:\nbins: [[0, 2.5, 20]]\n</code></pre> Setting Type Explanation <code>target</code> <code>str</code> The resampling is done in such a way that the distribution of the kinematic variables matches the distribution of those in one particular flavour given in here. Usually it is the leat populated flavour, as this flavour will not be resampled instead all jets of this flavour are taken. <code>method</code> <code>str</code> Either  <code>pdf</code>, <code>countup</code> or <code>none</code>, depending on the method you would like to use <code>upscale_pdf</code> <code>int</code> Optional only availabe for <code>pdf</code> preprocessing. The coarse approximation of the pdf functions based on histograms are interpolated and to bins that are upscale_pdf**dimensions times smaller than original <code>sampling_fraction</code> <code>None</code>, <code>float</code> or <code>auto</code> The number of the jets sampled from each batch is equal to the sampling fraction time number of the jets in input batch (after the curs and flavour selection). The large is this variable, the more are jets upsampled i.e. repeated, thus smaller values are prefered. On the other hand eith smaller sampling fractions lead to longer preprocesing times. <code>auto</code> option gives the smallest resampling fraction for each component depending on the number of available jets and number of jets that is asked for but caps it from below at 0.1 to prevent long preprocessing times when enough statistic is present. <code>variables</code> <code>dict</code> The jets will be resampled according to the distribution of the kinematic variables you provide here. The variable names must correspond to the ones in TDD. For each variable prlease provide a <code>bins</code> setting with a list of lists of 2 floats and a an integer each. Each of the sub lists represent a binning region and is described by lower bound upper bound and the number of bins of equal width in this regions. The bins from each region will be combined to provide one (heterogenous width) binning. When upscaling the pdf each bin region is upscaled separately. THerefore is not necessary but advisable to have a split in binnings at the same place where the cut betwenn regions takes place to better handle the discontinuities."},{"location":"configuration/#global-config","title":"Global Config","text":""},{"location":"configuration/#upp.classes.preprocessing_config.PreprocessingConfig","title":"<code>upp.classes.preprocessing_config.PreprocessingConfig</code>  <code>dataclass</code>","text":"<p>Global options for the preprocessing.</p> <p>These options are specified in the config file under the <code>global:</code> key. They are passed as kwargs to PreprocessingConfig. The config file is also copied to the output directory.</p> <p>For example: <pre><code>global:\njets_name: jets\nbatch_size: 1_000_000\nnum_jets_estimate: 5_000_000\nbase_dir: /my/stuff/\nntuple_dir: h5-inputs # resolved path: /my/stuff/h5-inputs/\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>config_path</code> <code>pathlib.Path</code> <p>Path to the config yaml file that is used. Does not need to be set in config.</p> <code>split</code> <code>upp.classes.preprocessing_config.Split</code> <p>For which part the preprocessing is run. Either train, val or test. This needs to be set as a command line argument when running the programm. Does not need to be set in config.</p> <code>config</code> <code>dict</code> <p>Dict with the loaded config. Does not need to be set in config.</p> <code>base_dir</code> <code>pathlib.Path</code> <p>Base directory for all other paths.</p> <code>ntuple_dir</code> <code>(pathlib.Path, optional)</code> <p>Directory containing the input h5 ntuples. If a relative path is given, it is interpreted as relative to base_dir. By default Path(\"ntuples\")</p> <code>components_dir</code> <code>(pathlib.Path, optional)</code> <p>Directory for intermediate component files. If a relative path is given, it is interpreted as relative to base_dir. By default Path(\"components\")</p> <code>out_dir</code> <code>(pathlib.Path, optional)</code> <p>Directory for output files. If a relative path is given, it is interpreted as relative to base_dir. By default Path(\"output\")</p> <code>out_fname</code> <code>(pathlib.Path, optional)</code> <p>Filename stem for the output files. By default Path(\"pp_output.h5\")</p> <code>batch_size</code> <code>(int, optional)</code> <p>Batch size for the preprocessing. For each batch select <code>sampling_fraction*batch_size_after_cuts</code>. It is recommended to choose high batch sizes especially to the <code>countup</code> method to achive best agreement of target and resampled distributions. By default 100_000</p> <code>num_jets_estimate</code> <code>(int, optional)</code> <p>Any of the further three arguments that are not specified will default to this value Is equal to 1_000_000 by default.</p> <code>num_jets_estimate_available</code> <code>(int | None, optional)</code> <p>A sabsample taken from the whole sample to estimate the number of jets after the cuts. Please keep this number high in order to not get poisson error of more then 5%. If time allows you can use -1 to get a precise number of jets and not just an estimate although it will be slow for large datasets. Is equal to num_jets_estimate by default.</p> <code>num_jets_estimate_hist</code> <code>(int | None, optional)</code> <p>Number of jets of each flavour that are used to construct histograms for probability density function estimation. Larger numbers give a better quality estmate of the pdfs. Is equal to num_jets_estimate by default.</p> <code>num_jets_estimate_norm</code> <code>(int | None, optional)</code> <p>Number of jets of each flavour that are used to estimate shifting and scaling during normalisation step. Larger numbers give a better quality estmates. Is equal to num_jets_estimate by default.</p> <code>num_jets_estimate_plotting</code> <code>(int | None, optional)</code> <p>Number of jets of each flavour used for plotting the initial and the final resampling variable distributions. Larger numbers give a better estimate of the full distributions. Is equal to num_jets_estimate by default.</p> <code>merge_test_samples</code> <code>(bool, optional)</code> <p>Merge the test samples of the different processes into one file. By default False.</p> <code>jets_name</code> <code>(str, optional)</code> <p>Name of the jets dataset in the input file. By default \"jets\".</p> <code>flavour_config</code> <code>(pathlib.Path | None, optional)</code> <p>Flavour config yaml file which is to be used. By default None</p> <code>flavour_category</code> <code>(str, optional)</code> <p>Flavour categories that are to be used. By default, the \"standard\" (non-extended) labels are loaded. The extended labels can be used by setting this value to \"extended\". By default \"standard\". To use this option, flavour_config must be None.</p> <code>num_jets_per_output_file</code> <code>(int | None, optional)</code> <p>Number of jets per final output file. If the number of total jets is larger than this number, the final h5 output files are splitted in multiple smaller files with this number of jets per file. By default None which produces one huge output file.</p> <code>skip_checks</code> <code>(bool, optional)</code> <p>Skip checks for the input files. This is used for grid submission</p> <code>skip_config_copy</code> <code>(bool, optional)</code> <p>Decide, if the config copying is skipped or not. By default False</p>"},{"location":"contributing/","title":"Contributing","text":"<p>If you find a bug, have a feature request or similar, feel free to submit an issue.</p>"},{"location":"contributing/#contributing-guidelines","title":"Contributing guidelines","text":"<p>If you want to contribute to the development of the UPP, you should create a fork of the repository. You can read about forking workflows here, or take a look at the contributing guidelines in the training dataset dumper documentation.</p> <p>You should make changes inside a feature branch in your fork. It is generally a good idea not to work directly on the the <code>main</code> branch in your fork. Then, when your feature is ready, open a merge request to the target branch on upstream (which will usually be <code>main</code>). Once this is merged to upstream, you can <code>git pull upstream main</code> from your fork to bring back in the changes, and then fork again off <code>main</code> to start the development of a new feature. If your feature branch becomes outdated with its target, you may have to rebase or merge in the changes from the target branch, and resolve any conflicts, before you can merge.</p> <p>Remember to keep you fork up to date with upstream.</p>"},{"location":"contributing/#code-formatting","title":"Code Formatting","text":"<p>It's good practice to document your code with module and function docstrings, and inline comments. Consider also providing type hints for the function in/outputs. It's also recommended to use black to format your contributions. You can take a look at the umami docs for more guidelines on code style.</p> <p>For the formatting in this project use pre-commit hooks:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>It is highly encoraged to adhere to provide unit and/or integration tests for every new added feature. You can test your code and check the coverage using. <pre><code>coverage run --source ftag -m pytest --show-capture=stdout\ncoverage report </code></pre> You may also find the codecov tool helpful</p>"},{"location":"reweighting/","title":"Reweighting","text":"<p>A different approach to balancing classes to resampling is to instead reweight them instead. In resampling, we bin our jets over their kinematics. We look in each bin, and if we have more jets of a given flavour compared to the target flavour, we throw those jets away. If we have less of a given flavour than the target flavour, we copy-and-paste (upsample) the jet until the bins are equal. This ensures that the flavour ratio of any two bins is approximately constant.</p> <p>This does however have some disadvantages: - results in throwing away some jets - upsampling introduces copies of the same jet, meaning we are passing the same data through the model more than once per epoch - bins always fave some non-zero finite size, such that there can still be a residual distibution shape, see this mention of shark-toothing</p> <p>We can instead reweight, where instead of throwing away or copying jets, we assign each jet in a bin a weight such that we end up with a constant weighted flavour ratio per bin.</p>"},{"location":"reweighting/#running-the-reweighting","title":"Running the reweighting","text":"<p>The reweighting consists of 4 main stages: - component splitting - histogram/weights calculation - merging - normalisation</p> <p>The normalisation step utilises the same commands as in resampling, and so is not discussed here further</p>"},{"location":"reweighting/#component-splitting","title":"Component Splitting","text":"<p>The first stage takes each sample and splits it into its split and flavour components. For example, if we have a ttbar sample, and we have 4 jet flavours (b,c,light,tau) then the splitting will produce 12 files: <code>train_bjets, val_bjets, test_bjets,...</code>.</p> <p>To run the splitting,  <pre><code>python upp/main.py --config {config} --split-components [--container {single container} --files {files}]\n</code></pre></p> <p>which will split the components. If <code>--container</code> is defined, then only that container from the config will be split. If this is not included, then all contains in the config will be split. Similarly, if <code>--files</code> is defined then only the specified files in the specified container will be split. Once the splitting is complete, a meta-data file is automatically created which points towards each file needed for the next stages of reweighting</p>"},{"location":"reweighting/#split-on-the-grid","title":"Split on the grid","text":"<p>The splitting stage can be quite time consuming, and is often limited by system IO. To speed the processing up, a grid-submission script is provided. First, you must setup a virtual environment to be packaged up and sent to the grid. This can be done by</p> <pre><code>setupATLAS\nsource upp/grid/setup_env.sh\n</code></pre> <p>Once this has setup, you can run the grid dump by:</p> <pre><code>python upp/grid/grid_split.py --config {config} --rucio_user {your rucio username} --tag {something to identify this dump} [--dryrun]\n</code></pre> <p>Where the dryrun flag will prepare the submission directory without submitting to the grid.</p> <p>Once all your jobs are complete, you can use the download_and_prepare script:</p> <pre><code>python upp/grid/download_and_prepare.py -config {config} --rucio_user {your rucio username} --tag {same tag as before}\n</code></pre> <p>which will then automatically download, package up, and generate the meta data ready for the next stage.</p>"},{"location":"reweighting/#generate-weights","title":"Generate weights","text":"<p>Once all the samples are prepared, we can calculate the weights. An example config can be found in <code>configs/GN3v01/GN3V01-RW.yaml</code>. We can look at the reweighting section:</p> <pre><code>reweighting:\nnum_jets_estimate: 1_500_000\nmerge_num_proc: 20\nreweights:\n- group: jets\nreweight_vars: [pt_btagJes, eta_btagJes]\nbins:\npt_btagJes:\n[\n[20_000, 250_000, 50],\n[250_000, 1_000_000, 50],\n[1_000_000, 6_000_000, 50],\n]\neta_btagJes: [[-2.5, 2.5, 40]]\nclass_var: flavour_label\nclass_target: mean\n</code></pre> <p><code>num_jets_estimate</code> represents the number of each jet flavour used to generate the reweighting histograms. The <code>merge_num_proc</code> variable will be relevent in the next section of these docs. Then, you have the <code>reweights</code> section, which includes a list of reweight configurations. In this example, we have the first reweight calculated over the jets group. It reweights based on the flavour-label, over the pt and eta distributions. The bins follow the same logic as in resampling. The class target can then either be chosen as a single label (e.g, if 0 then the reweighting would target the distribution for <code>flavour_label==0</code>), or one of <code>mean, min, max</code> which will instead target either the mean distribution, or always take the maximum/minumum bin counts as the target. The reweighting can also be performed over track variables, for example</p> <pre><code>group: jets\nreweight_vars: [pt_frac]\nbins:\npt_frac:\n[\n[0, 0.5, 50],\n[0.5, 1.0, 20]\n]\nclass_var: ftagTruthOriginLabel\nclass_target: mean\n</code></pre> <p>Would calculate weights such that we have equivilent pt_frac distributions across the track labels.</p> <p>To run the reweighting simply do</p> <pre><code>python upp/main.py --config {config} --rw\n</code></pre>"},{"location":"reweighting/#merging","title":"Merging","text":"<p>Finally, we can merge all the relevent jets with their weights. This is done by</p> <pre><code>python upp/main.py --rwm --split {train/test/val}\n</code></pre> <p>This can either work in series to create 1 single large file, or we can produce multiple files with multi-processing. To do this, ensure the <code>global</code> section of the pre-processing config includes <code>num_jets_per_output_file</code> and the <code>reweighting</code> section has <code>merge_num_proc&gt;1</code>. This will then launch <code>merge_num_proc</code> processes, with approximately <code>num_jets_per_output_file</code> per file*.</p> <ul> <li>Due to the nature of the H5Reader, the actual number of jets per file will be slightly smaller than what is requested, on the order of 0.1%.</li> </ul>"},{"location":"run/","title":"Run","text":"<p>Before running UPP, make sure you have modified the configuration file according to the configuration instructions</p>"},{"location":"run/#basic-usage","title":"Basic Usage","text":"<p>To run all preprocessing stages for the <code>train</code> split use:</p> <pre><code>preprocess --config configs/test.yaml\n</code></pre> <p>For a comprehensive list of available flags, refer to <code>preprocess --help</code>.</p> <p>If you are running on lxplus you may need to use <code>python3 upp/main.py</code> instead of <code>preprocess</code></p>"},{"location":"run/#splits","title":"Splits","text":"<p>The data is divided into three splits: training (<code>train</code>), validation (<code>val</code>), and testing (<code>test</code>). These splits are defined in configuration files, typically based on the <code>eventNumber</code> variable. By default, the train split contains 80% of the jets, while val and test contain 10% each.</p> <p>If you want to preprocess the <code>val</code> or <code>test</code> split, use the <code>--split</code> argument:</p> <pre><code>preprocess --config configs/config.yaml --split val\n</code></pre> <p>You can also process <code>train</code>, <code>val</code>, and <code>test</code> with a single command using <code>--split=all</code>.</p>"},{"location":"run/#stages","title":"Stages","text":"<p>The preprocessing is broken up into several stages.</p> <p>To run with only specific stages enabled, include the flag for the required stages:</p> <pre><code>preprocess --config configs/config.yaml --prep --resample\n</code></pre> <p>To run the whole chain excluding certain stages, include the corresponding negative flag (<code>--no-*</code>). For example to run without plotting</p> <pre><code>preprocess --config configs/config.yaml --no-plot\n</code></pre> <p>The stages are described below.</p>"},{"location":"run/#1-prepare","title":"1. Prepare","text":"<p>The prepare stage (<code>--prep</code>) checks first the number of inital jets that are available per group/sample. For each of the entries in the <code>pattern</code> of the group, it checks how many jets are in total available. If this differs too much between the entries in <code>pattern</code>, an error is thrown because it indicates that you will might introduce biases in the training. For example, usually entries in <code>pattern</code> are different MC campaigns and by using drastically different numbers of initial jets, a campaign dependency can be introduced. If you manually checked it and you expect large differences, you can skip this by adding the command line argument <code>--skip-sample-check</code>. If you run the script the first time and you want to run the prepare stage in parallel, please let this script run first! It creates virtual datasets for each entry in <code>pattern</code> which could become corrupted if you do run this script in parallel multiple times! Instructions on how to run this check stand-alone can be found in here.</p> <p>Afterwards, the prepare stage reads a specified number of jets (<code>num_jets_estimate_hist</code>) for each flavor and constructs histograms of the resampling variables. These histograms are stored in <code>&lt;base_dir&gt;/hists</code>.</p> Paralellisation <p>This step can be paralellised to speed up the histogram creation. To do so, you need to provide the additional <code>--component</code> flag. The argument for the flag is the name of the component, which is to be processed. The argument can be constructed when looking closer at the different blocks in the <code>components</code> part of the config file. As an example, we take the <code>ghost-highstat.yaml</code> config file from the <code>gn3</code> folder in <code>configs/</code>:</p> <pre><code>- region:\n&lt;&lt;: *lowpt\nsample:\n&lt;&lt;: *ttbar\nflavours: [ghostsplitbjets]\nnum_jets: 22_000_000\nnum_jets_test: 2_000_000\n</code></pre> <p>The argument for the component flag can be constructed by taking the name of the region (this is defined in the definition of <code>lowpt</code>)</p> <pre><code>lowpt: &amp;lowpt\nname: lowpt\ncuts:\n- [pt_btagJes, \"&gt;\", 20_000]\n- [pt_btagJes, \"&lt;\", 250_000]\n</code></pre> <p>plus the name of the sample which is used (this is defined in the definition of <code>ttbar</code>)</p> <pre><code>ttbar: &amp;ttbar\nname: ttbar\nequal_jets: False\npattern:\n- \"user.svanstro.601589.e8547_s3797_r13144_p6368.tdd.GN3_dev.25_2_27.24-09-17_v00_output.h5/*.h5\" # mc20d\n- \"user.svanstro.601589.e8549_s4159_r14799_p6368.tdd.GN3_dev.25_2_27.24-09-17_v00_output.h5/*.h5\" # mc23a\n</code></pre> <p>and finally the flavour that is used. In this case, <code>ghostsplitbjets</code>. The full name of the component is therefore: <code>lowpt_ttbar_ghostsplitbjets</code>. The full command would look like this:</p> <pre><code>preprocess --config configs/config.yaml --prep --component lowpt_ttbar_ghostsplitbjets\n</code></pre> <p>It is hardly discouraged to run multiple steps with this option enabled. This option is mainly to paralellise the processing on HPCs. In addition, do not run this in the same job with multiple threads! h5py has access issues when the same file is read by multiple threads in the same job. Use multiple instances/jobs to run this.</p>"},{"location":"run/#2-resample","title":"2. Resample","text":"<p>The resample stage (<code>--resample</code>) resamples jets to achieve similar p_T and \\eta\\eta distributions across flavours. After execution, resampled samples for each flavor, sample, and split are saved separately in <code>&lt;base_dir&gt;/components/&lt;split&gt;/</code>. You need to run the resampling stage even if you don't apply any resampling (e.g. you configured with <code>method: none</code>).</p> Paralellisation <p>Similar to the <code>--prep</code> step, the resampling step is also able to run in parallel, but only for the different region (e.g. <code>lowpt</code> &amp; <code>highpt</code>). To do so, you need to run with the command line argument <code>--region</code> which takes as input the region on which to run. Please ensure that all components for this region were prepared in the <code>--prep</code> step before running this!</p> <p>The command to run the specific region would look like this:</p> <pre><code>preprocess --config configs/config.yaml --resample --region lowpt\n</code></pre> <p>Similar to the <code>--prep</code> step, it is hardly discouraged to run multiple steps with this option enabled. This option is mainly to paralellise the processing on HPCs. Once all regions are resampled, you can continue with the following steps.</p> <p>If you want to go one step further, you can also tell the resampling to resample each component in the region in it's own process. To do so, you need to provide the <code>--region</code> command line argument together with the <code>--component</code> command line argument. Very important is here the full name of the component, which is constructed in the same way as already explained in the paralellisation chapter of the Prepare stage.</p> <p>The command to run the specific region would look like this:</p> <pre><code>preprocess --config configs/config.yaml --resample --region lowpt --component lowpt_ttbar_ghostsplitbjets\n</code></pre> <p>Similar to the <code>--prep</code> step and the previous <code>--region</code> explanation, it is hardly discouraged to run multiple steps with this option enabled. This option is mainly to paralellise the processing on HPCs. Once all components from all regions are resampled, you can continue with the following steps. Furthermore, do not run this in the same job with multiple threads! h5py has access issues when the same file is read by multiple threads in the same job. Use multiple instances/jobs to run this. Also, please do NOT use this functionality if you don't have fast I/O (Harddrives). This is very heavy in terms of I/O load and ends up to be slower if you are using \"default\" HDD drives.</p>"},{"location":"run/#3-merge","title":"3. Merge","text":"<p>The merge stage (<code>--merge</code>) combines the resampled samples into a single file named <code>&lt;tbase_dir&gt;/&lt;out_dir&gt;/pp_output_&lt;split&gt;.h5</code>. It also handles shuffling.</p>"},{"location":"run/#4-normalise","title":"4. Normalise","text":"<p>The normalise stage (<code>--norm</code>) calculates scaling and shifting values for all variables intended for training based on (<code>num_jets_estimate_norm</code>). The results are stored in<code>&lt;tbase_dir&gt;/&lt;out_dir&gt;/norm_dict.yaml</code>.</p>"},{"location":"run/#5-plotting","title":"5. Plotting","text":"<p>The plotting stage (<code>--plot</code>) produces histograms of resampled variables to verify the resampling quality. You can find these plots in <code>&lt;tbase_dir&gt;/plots/</code>.</p>"},{"location":"run/#additional-scripts-initial-sample-check","title":"Additional Scripts: Initial Sample Check","text":"<p>The check for the inital samples from the prepare stage can also be run stand-alone. This is important if you plan to run in parallel mode. To do so, you can simply use the following command:</p> <pre><code>check_input_samples --config_path &lt;path/to/your/config&gt;\n</code></pre> <p>You can also add the <code>--deviation-factor</code>, which is by default <code>10.0</code> and the <code>--verbose</code> flags. The latter will print the number of inital jets to your terminal.</p>"},{"location":"sampling/","title":"Sampling methods","text":""},{"location":"sampling/#pdf-probability-density-function","title":"PDF (probability density function)","text":"<p>This is a implementation of the importance sampling.  It is done vi following algorythm:</p> <ol> <li><code>num_jets_estimate</code> jets are binned for each flavour using the configurations for resampling variable bins. This histogram is the initial estimate of the pdf of jets of each flavour.</li> <li>The importance function is estimated using the ratio of the histograms <code>pdf_target_flavour/pdf_resampled_flavour</code> for each flavor except the target. Safe divesion is used i.e. if <code>pdf_resampled_flavour</code> is 0 the expressin defaults thus the jets where <code>pdf_target_flavour</code> or <code>pdf_resampled_flavour</code> is estimated by 0 are not resampled. </li> <li>Optionally the importance function is upscaled i.e. interpolated using cubic spline interpolation to a finer grid of bins. Centres of bins are used as nodes for the splines and the function is evaluated in the centeres of the new bins where the new bins are created by splitting the old bins in <code>upscale_pdf</code> intervals of equal width. This way the edge bins of each binning region are actually extrapolated rather than interpolated.</li> <li>The new batch of jets is being read and after the cuts are applied <code>n_batch</code> jets remain. The jets are binned with the the binning from step 1 (if upscaling is not used) or upscaled binning defined by 3 (if upscaling is used) and the reference number of the bin for each jet is saved.</li> <li>Each jet is assigned an importance score equal to the value of the importance function in the corresponding bin.</li> <li><code>n_batch*flavour.sampling_fraction</code> jets are selected with replacement using importances as weights.</li> </ol> <p>This algryhtm is used for all the flavours except the target flavour for which all jets are saved without sampling as they already follow the desired distribution. One has to remember that <code>flavour.sampling_fraction==1</code> will lead to many jets beig selected more then once, choosing lower <code>sampling_fractions</code> can help agains it.</p>"},{"location":"sampling/#countup","title":"Countup","text":"<p>Countup resampling tries to select as many unique jets from each bin as possible before selecting the duplicates.</p> <ol> <li><code>num_jets_estimate</code> jets are binned for each flavour using the configurations for resampling variable bins. This histogram is the initial estimate of the pdf of jets of each flavour.</li> <li>The new batch of jets is being read and after the cuts are applied <code>n_batch</code> jets remain. The jets are binned with the the binning from step 1 (if upscaling is not used) or upscaled binning defined by 3 (if upscaling is used) and the reference number of the bin for each jet is saved.</li> <li>The number of requested jets in each bin are calculated as <code>floor(n_batch*pdf_target_flavour+uniform([0, 1]))</code> so that if <code>n_batch*flavour.sampling_fraction*pdf_target_flavour=1.2</code> it has a 80% chance to be rounded up to 1 and 20% cnahce to be rounded up to 2 so that for each bin we get an integer number that on average corresponds to the expected value. </li> <li>From each bin we select consecutively (without replacement) the required number of jets. If the bin holds less jets than the requested number the rest of jets in this bin is chosen at random from this bin with replacemene. This way only few jets in each bin are repeated for <code>flavour.sampling_fraction=1</code> and rarely any are repeated for smaller sampling fractions</li> <li>It may happen that we required jets from the bin that is empty in this batch thus the operations above lead to less jets than requested in total. To compencate we resample this number at random (with replacement) from already sampled jets. This leads to some additional repetitions but this way we can be sure that we adhere to the target pdf.</li> </ol>"},{"location":"setup/","title":"Setup","text":"<p>This guide will walk you through the process of setting up the Umami-Preprocessing Python package on your system.</p>"},{"location":"setup/#environment-setup","title":"Environment Setup","text":"<p>Creating a virtual environment helps keep your Python environment isolated and organized. While it's not essential, we highly recommend using one. You can set up a virtual environment using either Conda or Python's venv. UPP requires Python 3.8 or later.</p> condavenvlxplus <p>Set up a fresh conda or mamba environment:</p> <pre><code>mamba create -n upp python=3.11\nmamba activate upp\n</code></pre> <p>venv is a lightweight solution for creating virtual python environments, however it is not as fully featured as a fully fledged package manager such as conda. Create a fresh virtual environment and activate it using</p> <pre><code>python3 -m venv env\nsource env/bin/activate\n</code></pre> <p>If you don't want to use conda environments on lxplus, you can setup python via</p> <pre><code>setupATLAS\nlsetup \"python 3.9.18-x86_64-el9\"\n</code></pre> <p>You can also set up conda on lxplus</p>"},{"location":"setup/#pypi-installation","title":"PyPi installation","text":"<p>A simple installation can be done via <code>pip</code> from Python Packade Index:</p> <pre><code>python -m pip install umami-preprocessing\n</code></pre> <p>On lxplus you may have to use <code>python3</code></p> <p>After this installation all the fuctionality for the user is available. The further steps are only useful for the development of the package.</p>"},{"location":"setup/#get-the-code","title":"Get the code","text":"<p>Start by cloning the Umami-Preprocessing repository and navigating into the project directory using the following commands in your terminal:</p> <pre><code>git clone https://github.com/umami-hep/umami-preprocessing.git\ncd umami-preprocessing\n</code></pre>"},{"location":"setup/#install-package-from-code","title":"Install package from code","text":"<p>Install the package as in editable mode if you would like to develop the code:</p> <pre><code>python -m pip install -e .[dev]\n</code></pre> <p>Or do a simple installation if you only plan to use the provided functionality as is</p> <pre><code>python -m pip install .\n</code></pre> <p>Note for running on lxplus</p> <p>Again, you may have to use <code>python3</code> here.</p> <p>You may also see a warning like this:</p> <pre><code>WARNING: The script preprocess is installed in '/afs/cern.ch/user/X/Y/.local/bin' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n</code></pre> <p>if you do, you can add the directory to your path using</p> <pre><code>export PATH=$PATH:/afs/cern.ch/user/X/Y/.local/bin\n</code></pre> <p>Alternatively, you can just run the scripts by pointing to the <code>main.py</code></p> <pre><code>python3 upp/main.py\n</code></pre>"},{"location":"setup/#run-the-tests-optional","title":"Run the tests (Optional)","text":"<p>To ensure that the package is working correctly, you can run tests using the pytest framework.  Additionally, you can install coverage to generate a coverage report.</p> <p>Use <code>pytest</code> to run the tests to make sure the package works</p> <pre><code>pytest tests </code></pre> <p>If you want to measure test coverage, first install coverage, and then run tests as follows:</p> <pre><code>coverage run --source upp -m pytest tests --show-capture=stdout\ncoverage report </code></pre>"},{"location":"umami_int/","title":"Umami integration","text":"<p>UPP is istalled alongside Umami and can be used from within the umami framework in a very simple manner</p>"},{"location":"umami_int/#umami-specific-configs","title":"Umami-specific configs","text":"<p>First you need to add umami-specific configs to the config file. Here is an example config: <pre><code>umami:\ngeneral:\nplot_name: PFlow_ext-hybrid\nplot_type: \"pdf\"\nuse_atlas_tag: True\natlas_first_tag: \"Simulation Internal\"\natlas_second_tag: \"$\\\\sqrt{s}=13$ TeV, PFlow jets\"\nlegend_sample_category: True\nvar_file: umami/user/upp_prep_small/config/Dips_Variables_R22.yaml\ndict_file: &lt;base_dir&gt;&lt;out_dir&gt;PFlow-scale_dict.json\ncompression: lzf\nprecision: float16\nconcat_jet_tracks: False\nsampling:\nuse_validation_samples: false\noptions:\nn_jets_to_plot: 3e4\nsave_tracks: true\nsave_track_labels: true\nbool_attach_sample_weights: false\ntracks_names: [\"tracks\"]\nn_jets_scaling: 0\n\nThis config part mimics the umami config structure. Parameters in `general` mimic ones that are in the root of umami config. Parameters in `sampling`, `sampling.options`, `parameters` and `convert_to_tfrecord` mimic the corresponding structures in umami config. All the parameters given in the example should be given in order for UPP integration in umami to work except `parameters` and `convert_to_tfrecord`. You need to provide `convert_to_tfrecord` if you need to convert dataset to TFrecord and `parameters` oonly if you does not want to saveinto `&lt;base_dir&gt;&lt;out_dir&gt;` by default.\nPlease refer to umami documentation [https://umami.docs.cern.ch/preprocessing/Overview/] for up-to-date explanation.\n\n### Running preprocessing\n\nAfter you make the necessary changes to the config file you can perform preprocessing in umami by running the umami/preprocessing.py script\nthe same way as you would do with the old umami preprocessing ```bash\ncd umami preprocessing.py --config_file path/to/my_upp_config.yaml --resampling\npreprocessing.py --config_file path/to/my_upp_config.yaml --scaling\npreprocessing.py --config_file path/to/my_upp_config.yaml --write\n</code></pre></p> <p>Umami will first <code>try</code> to read the config file as an old umami preprocessing configuration. When that fails it will read the config as a UPP preprocessing config.</p> <ul> <li><code>--resampling</code> step will perform Upp preprocessing with this config file and <code>split==all</code></li> <li><code>--scaling</code> step will execute umami version of rescaling code that will prepare a json scaling dictionary at <code>dict_file</code> location</li> <li><code>--write</code> step will execute umamii code for scaling the variables and writing them in an unstructured scaling array it will also produce default umami preprocessing plots</li> <li><code>--to_records</code> step will execute umamii code for converting dataset to a TFrecords format </li> </ul> <p>Note: <code>--prepare</code> step will do nothing and will only trow an arror as UPP does not require (same) preparation as old umami preprocession.  <code>--resampling --hybrid_validation</code> is also not available for upp as it does both splits at the resampling step</p> <p>After that one can use the results of the preprocessing for umami trainig for example for DL1 or DIPS. One can either only run <code>--resampling</code> and <code>--scaling</code> and train on the structured array data using TDDgenerator by setting your training configs similar to this: <pre><code># Set modelname and path to Pflow preprocessing config file\nmodel_name: user_DL1r-PFlow_new-taggers-stats-22M-tdd-upp\npreprocess_config: /home/users/o/oleksiyu/WORK/umami/user/upp_prep_small/config/upp_prepr.yaml\n\n# Add here a pretrained model to start with.\n# Leave empty for a fresh start\nmodel_file: \n\n# Add training file\ntrain_file: &lt;base_dir&gt;&lt;out_dir&gt;pp_output_train.h5\n\n# Defining templates for the variable cuts\n...\n\n#Add validation files\nvalidation_files:\n    r22_hybrid_val:\n        path: &lt;base_dir&gt;&lt;out_dir&gt;pp_output_val.h5\n        label: \"Hybrid Validation\"\n\ntest_files:\n    ttbar_r22:\n        path: &lt;base_dir&gt;&lt;out_dir&gt;pp_output_test_ttbar.h5\n        &lt;&lt;: *variable_cuts_ttbar\n\n    zpext_r22:\n        path: &lt;base_dir&gt;&lt;out_dir&gt;pp_output_test_zprime.h5\n        &lt;&lt;: *variable_cuts_zpext\n</code></pre> or you can perform all three steps to train using unstructured array data. This way one looses time to write the dataset but the training may be somewhat faster.  To do this just chnge your training_file in the example above to  <pre><code>train_file: &lt;base_dir&gt;&lt;out_dir&gt;pp_output_train_resampled_scaled_shifted.h5\n</code></pre></p>"}]}